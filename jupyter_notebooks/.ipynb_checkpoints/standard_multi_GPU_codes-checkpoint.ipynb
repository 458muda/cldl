{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Multi-GPU and distributed training in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook is to show how to use multiple GPUs to train a model in keras. This is also called distributed training. Here is the [link](https://keras.io/guides/distributed_training/). For this tutorial, we are applying data parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There is a specific version for tensorflow (2.2.0+) and keras to implement the strategy. Just for reference, the version I used for the experiment is:\n",
    "Python==3.6.8\n",
    "tensorflow-gpu==2.2.0\n",
    "keras==2.3.1\n",
    "\n",
    "Be careful when you use the conda to install the tensorflow-gpu. The default version is 2.1.0. You need to use the command line to download the specific version. Like\n",
    "```\n",
    "conda install tensorflow-gpu=2.2.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### import libraries and set the GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Import the important libraries\n",
    "# you need to import gdal, tensorflow_gpu, numpy, matplotlib, Pillow, keras, segmentation_models\n",
    "from osgeo import gdal\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.utils import multi_gpu_model, plot_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "import segmentation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"0, 1, 2, 3\"\n",
    "print(tf.test.is_gpu_available())\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def scene_folder_list(catagory, pathrow_list, data_source_folder='/workspace/_libs/dl_library'):\n",
    "    source_folder = os.path.join(data_source_folder,catagory)\n",
    "    res_folder_list = []\n",
    "    total_num = len(pathrow_list)\n",
    "    success_num = 0\n",
    "    for pathrow in pathrow_list:\n",
    "        for folder in os.listdir(source_folder):\n",
    "            if folder.endswith(pathrow):\n",
    "                res_folder_list.append(os.path.join(source_folder,folder))\n",
    "                success_num+=1\n",
    "    print(str(success_num),'scenes exist for',str(total_num),'required scenes')\n",
    "    return res_folder_list\n",
    "\n",
    "def read_scenes(scene_folder_list):\n",
    "    for i in range(len(scene_folder_list)):\n",
    "        if(i==0):\n",
    "            folder = scene_folder_list[i]\n",
    "            res_bands, res_label = read_scene_band_label(folder)\n",
    "        else:\n",
    "            folder = scene_folder_list[i]\n",
    "            scene_bands, scene_label = read_scene_band_label(folder)\n",
    "            res_bands = np.append(res_bands, scene_bands, axis=0)\n",
    "            res_label = np.append(res_label, scene_label, axis=0)\n",
    "    return res_bands, res_label\n",
    "\n",
    "\n",
    "def read_scene_band_label(scene_folder):\n",
    "    \"\"\"\n",
    "This function is to read bands and label from the\n",
    "    :param scene_folder: the folder path of a scene\n",
    "    :return: numpy arrays of bands and the label\n",
    "    \"\"\"\n",
    "    # empty list to save the band and label\n",
    "    band_array = []\n",
    "    label_array = []\n",
    "    # get the band and label folder\n",
    "    for folder in os.listdir(scene_folder):\n",
    "        if(folder.endswith('bands')):\n",
    "            scene_band_folder = os.path.join(scene_folder,folder)\n",
    "        if(folder.endswith('label')):\n",
    "            scene_label_folder = os.path.join(scene_folder,folder)\n",
    "    # save each band chips to the list\n",
    "    band_list = os.listdir(scene_band_folder)\n",
    "    label_list = os.listdir(scene_label_folder)\n",
    "    band_list.sort()\n",
    "    label_list.sort()\n",
    "    for i in band_list:\n",
    "        file = gdal.Open(os.path.join(scene_band_folder,i))\n",
    "        res_array = np.zeros((256, 256, 6))\n",
    "        for num in range(6):\n",
    "            array = np.array(file.GetRasterBand(num+1).ReadAsArray())\n",
    "            res_array[:,:,num] = array\n",
    "        band_array.append(res_array)\n",
    "        del file\n",
    "    # save each label chip to the list\n",
    "    for i in label_list:\n",
    "        file = gdal.Open(os.path.join(scene_label_folder,i))\n",
    "        array = np.array(file.GetRasterBand(1).ReadAsArray())\n",
    "        label_array.append(array)\n",
    "        del file\n",
    "    return np.array(band_array), np.array(label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "# import random\n",
    "# def data_generator(x, y, batch_size, shuffle_data=True):\n",
    "#     num_data = len(x)\n",
    "#     while True:\n",
    "#         if(shuffle_data):\n",
    "#             x, y = shuffle(x, y)\n",
    "#         # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n",
    "#         for offset in range(0, num_data, batch_size):\n",
    "#             # Get the samples you'll use in this batch\n",
    "#             x_sample = x[offset:offset+batch_size]\n",
    "#             y_sample = y[offset:offset+batch_size]\n",
    "#             # Initialise X_train and y_train arrays for this batch\n",
    "#             x_train = []\n",
    "#             y_train = []\n",
    "#             for i in range(len(x_sample)):\n",
    "#                 img = x_sample[i]\n",
    "#                 label = y_sample[i]\n",
    "#                 ## preprocess images\n",
    "#                 ## random rotate\n",
    "#                 random_idx=random.randint(0, 3)\n",
    "#                 img = np.rot90(img,random_idx)\n",
    "#                 label = np.rot90(label,random_idx)\n",
    "#                 ## random flip\n",
    "#                 h_flip = random.choice([True, False])\n",
    "#                 v_flip = random.choice([True, False])\n",
    "#                 if(h_flip):\n",
    "#                     img = np.fliplr(img)\n",
    "#                     label = np.fliplr(label)\n",
    "#                 if(v_flip):\n",
    "#                     img = np.flipud(img)\n",
    "#                     label = np.flipud(label)\n",
    "\n",
    "#                 # Add example to arrays\n",
    "#                 x_train.append(img)\n",
    "#                 y_train.append(label)\n",
    "#             # Make sure they're numpy arrays (as opposed to lists)\n",
    "#             x_train = np.array(x_train)\n",
    "#             y_train = np.array(y_train)\n",
    "\n",
    "#             # The generator-y part: yield the next training batch\n",
    "#             yield x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# rotation for 90, 180, 270, and horizotal and vertical flip\n",
    "def clarkAug(bands, label):\n",
    "    ## rotation 1 for 90, 2 for 180, 3 for 270\n",
    "    image_rot90 = np.rot90(bands, 1, axes=(1, 2))\n",
    "    label_rot90 = np.rot90(label, 1, axes=(1, 2))\n",
    "    image_rot180 = np.rot90(bands, 2, axes=(1, 2))\n",
    "    label_rot180 = np.rot90(label, 2, axes=(1, 2))\n",
    "    image_rot270 = np.rot90(bands, 3, axes=(1, 2))\n",
    "    label_rot270 = np.rot90(label, 3, axes=(1, 2))\n",
    "    # axis=1 vertical flip, axis=2 horizontasl flip\n",
    "    image_vflip = np.flip(bands, axis=1)\n",
    "    label_vflip = np.flip(label, axis=1)\n",
    "    image_hflip = np.flip(bands, axis=2)\n",
    "    label_hflip = np.flip(label, axis=2)\n",
    "    res_bands = np.vstack([bands, image_rot90, image_rot180, image_rot270, image_vflip, image_hflip])\n",
    "    res_label = np.vstack([label, label_rot90, label_rot180, label_rot270, label_vflip, label_hflip])\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(res_bands)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(res_label)\n",
    "    return res_bands, res_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Activation, MaxPooling2D, Conv2D, Conv2DTranspose, concatenate, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers.experimental import SyncBatchNormalization\n",
    "\n",
    "def binary_unet(input_height, input_width, bandNum):\n",
    "    inputs = Input((input_height,input_width, bandNum))\n",
    "    # Block one\n",
    "    # (256, 256, numBands) -> (256, 256, 64)\n",
    "    conv1 = SyncBatchNormalization()(Conv2D(64, 3, padding='same', name='Conv1_1', kernel_initializer='he_normal')(inputs))\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = SyncBatchNormalization()(Conv2D(64, 3, padding='same', name='Conv1_2', kernel_initializer='he_normal')(conv1))\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    # (256, 256, 64) -> (128, 128, 64)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    pool1 = Dropout(0.2)(pool1)\n",
    "\n",
    "    # Block Two\n",
    "    # (128, 128, 64) -> (128, 128, 128)\n",
    "    conv2 = SyncBatchNormalization()(Conv2D(128, 3, padding='same', name='Conv2_1', kernel_initializer='he_normal')(pool1))\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = SyncBatchNormalization()(Conv2D(128, 3, padding='same', name='Conv2_2', kernel_initializer='he_normal')(conv2))\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    # (128, 128, 128) -> (64, 64, 128)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = Dropout(0.2)(pool2)\n",
    "\n",
    "    # Block three\n",
    "    # (64, 64, 128) -> (64, 64, 256)\n",
    "    conv3 = SyncBatchNormalization()(Conv2D(256, 3, padding='same', name='Conv3_1', kernel_initializer='he_normal')(pool2))\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = SyncBatchNormalization()(Conv2D(256, 3, padding='same', name='Conv3_2', kernel_initializer='he_normal')(conv3))\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    # (64, 64, 256) -> ( 32, 32, 256)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = Dropout(0.2)(pool3)\n",
    "\n",
    "    # Block four\n",
    "    # (32, 32, 256) -> (32, 32, 512)\n",
    "    conv4 = SyncBatchNormalization()(Conv2D(512, 3, padding='same', name='Conv4_1', kernel_initializer='he_normal')(pool3))\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = SyncBatchNormalization()(Conv2D(512, 3, padding='same', name='Conv4_2', kernel_initializer='he_normal')(conv4))\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    # (32, 32, 512) -> (16, 16, 512)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = Dropout(0.2)(pool4)\n",
    "\n",
    "    # Block five\n",
    "    # (16, 16, 512) -> (16, 16, 1024)\n",
    "    conv5 = SyncBatchNormalization()(Conv2D(1024, 3, padding='same', name='Conv5_1', kernel_initializer='he_normal')(pool4))\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = SyncBatchNormalization()(Conv2D(1024, 3, padding='same', name='Conv5_2', kernel_initializer='he_normal')(conv5))\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Dropout(0.2)(conv5)\n",
    "\n",
    "    ## Decoder\n",
    "    # Block six\n",
    "    # (16, 16, 1024) -> (32, 32, 1024) -> (32, 32, 512)\n",
    "    up6 = Conv2DTranspose(512, (4, 4), strides=(2, 2), name = 'Conv6_1', padding='same')(conv5)\n",
    "    # (32, 32, 512) -> (32, 32, 1024)\n",
    "    merge6 = concatenate([conv4,up6],axis = 3)\n",
    "    # (32, 32, 1024) -> (32, 32, 512)\n",
    "    conv6 = (Conv2D(512, 3,  padding = 'same', name = 'Conv6_2', kernel_initializer = 'he_normal')(merge6))\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = (Conv2D(512, 3, padding = 'same', name = 'Conv6_3', kernel_initializer = 'he_normal')(conv6))\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "\n",
    "    # Block seven\n",
    "    # (32, 32, 512) -> (64, 64, 512) -> (64, 64, 256)\n",
    "    up7 = Conv2DTranspose(256, (4, 4), strides=(2, 2), name = 'Conv7_1', padding='same')(conv6)\n",
    "    # (64, 64, 256) -> (64, 64, 512)\n",
    "    merge7 = concatenate([conv3,up7],axis = 3)\n",
    "    merge7 = Dropout(0.4)(merge7)\n",
    "    # (64, 64, 512) -> (64, 64, 256)\n",
    "    conv7 = (Conv2D(256, 3,  padding = 'same', name = 'Conv7_2', kernel_initializer = 'he_normal')(merge7))\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = (Conv2D(256, 3, padding = 'same', name = 'Conv7_3', kernel_initializer = 'he_normal')(conv7))\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "\n",
    "    # Block eight\n",
    "    # (64, 64, 256) -> (128, 128, 256) -> (128, 128, 128)\n",
    "    up8 = Conv2DTranspose(128, (4, 4), strides=(2, 2), name = 'Conv8_1', padding='same')(conv7)\n",
    "    # (128, 128, 128) -> (128, 128, 256)\n",
    "    merge8 = concatenate([conv2,up8],axis = 3)\n",
    "    merge8 = Dropout(0.4)(merge8)\n",
    "    # (128, 128, 256) -> (128, 128, 128)\n",
    "    conv8 = (Conv2D(128, 3, padding = 'same', name = 'Conv8_2', kernel_initializer = 'he_normal')(merge8))\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = (Conv2D(128, 3,  padding = 'same', name = 'Conv8_3', kernel_initializer = 'he_normal')(conv8))\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "\n",
    "    # Block nine\n",
    "    # (128, 128, 128) -> (256, 256, 128) -> (256, 256, 64)\n",
    "    up9 = Conv2DTranspose(64, (4, 4), strides=(2, 2), name = 'Conv9_1', padding='same')(conv8)\n",
    "    # (256, 256, 64) -> (256, 256, 128)\n",
    "    merge9 = concatenate([conv1,up9],axis = 3)\n",
    "    merge9 = Dropout(0.5)(merge9)\n",
    "    # (256, 256, 128) -> (256, 256, 64)\n",
    "    conv9 = (Conv2D(64, 3, padding = 'same',name = 'Conv9_2', kernel_initializer = 'he_normal')(merge9))\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = (Conv2D(64, 3, padding = 'same', name = 'Conv9_3', kernel_initializer = 'he_normal')(conv9))\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "\n",
    "    # (256, 256, 64) -> (256, 256, 64)\n",
    "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "    model = Model(inputs, outputs = conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def my_init(shape, dtype = tf.float32):\n",
    "    filter_size = 4\n",
    "    num_channels = 1\n",
    "    bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)\n",
    "    scale_factor = (filter_size + 1) // 2\n",
    "    if filter_size % 2 == 1:\n",
    "        center = scale_factor - 1\n",
    "    else:\n",
    "        center = scale_factor - 0.5\n",
    "    for x in range(filter_size):\n",
    "        for y in range(filter_size):\n",
    "            bilinear_kernel[x, y] = (1 - abs(x - center) / scale_factor) * \\\n",
    "                                    (1 - abs(y - center) / scale_factor)\n",
    "    bilinear_weights = np.zeros((filter_size, filter_size, num_channels, num_channels))\n",
    "    for i in range(num_channels):\n",
    "        bilinear_weights[:, :, i, i] = bilinear_kernel\n",
    "    bilinear_init = tf.keras.initializers.Constant(bilinear_weights)\n",
    "    return bilinear_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def binary_FCN2( input_height, input_width, bandNum):\n",
    "    ## input_height and width must be devisible by 32 because maxpooling with filter size = (2,2) is operated 5 times,\n",
    "    ## which makes the input_height and width 2^5 = 32 times smaller\n",
    "    assert input_height%32 == 0\n",
    "    assert input_width%32 == 0\n",
    "    IMAGE_ORDERING =  \"channels_last\"\n",
    "\n",
    "    img_input = Input(shape=(input_height,input_width, bandNum)) ## Assume 224,224,3\n",
    "\n",
    "    ## Block 1\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='block1_conv1', data_format=IMAGE_ORDERING )(img_input)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', name='block1_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format=IMAGE_ORDERING )(x)\n",
    "    pool1 = x #120x120x64\n",
    "\n",
    "    # Block 2\n",
    "#     x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "#     x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', name='block2_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', name='block2_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format=IMAGE_ORDERING )(x)\n",
    "    pool2 = x #120x120x128\n",
    "\n",
    "    # Block 3\n",
    "#     x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "#     x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "#     x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', name='block3_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format=IMAGE_ORDERING )(x)\n",
    "    pool3 = x #56x56x256\n",
    "\n",
    "    # Block 4\n",
    "#     x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "#     x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "#     x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block4_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format=IMAGE_ORDERING )(x)## (None, 14, 14, 512)\n",
    "    pool4 = x\n",
    "    #28x28x512\n",
    "    # Block 5\n",
    "#     x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "#     x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "#     x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block5_conv1', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block5_conv2', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(512, (3, 3), padding='same', name='block5_conv3', data_format=IMAGE_ORDERING )(x)\n",
    "    x = SyncBatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    pool5 = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format=IMAGE_ORDERING )(x)## (None, 7, 7, 512)\n",
    "\n",
    "\n",
    "#     vgg  = Model(  img_input , pool5  )\n",
    "#     vgg.load_weights(VGG_Weights_path) ## loading VGG weights for the encoder parts of FCN8\n",
    "\n",
    "    n = 4096\n",
    "    o = ( Conv2D( n , ( 7 , 7 ) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING))(pool5)\n",
    "    predict0 = ( Conv2D( n , ( 1 , 1 ) , activation='relu' , padding='same', name=\"predict0\", data_format=IMAGE_ORDERING))(o) # my prefered\n",
    "    predict1 = ( Conv2D( 1 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"predict1\", data_format=IMAGE_ORDERING))(predict0)\n",
    "\n",
    "#     predict1 = ( Conv2D( n , ( 1 , 1 ) , activation='relu' , padding='same', name=\"predict1\", data_format=IMAGE_ORDERING))(o)\n",
    "\n",
    "\n",
    "    ## 4 times upsamping for pool4 layer\n",
    "#     Deconv1 = Conv2DTranspose( nClasses , kernel_size=(3,3) , padding = 'same', strides=(2,2) , use_bias=False, data_format=IMAGE_ORDERING )(predict1)\n",
    "    Deconv1 = Conv2DTranspose( 1 , kernel_size=(4,4) , padding = 'same', strides=(2,2) , use_bias=False, kernel_initializer=my_init, data_format=IMAGE_ORDERING )(predict1)\n",
    "#     Deconv1 = UpSampling2D(size=(2, 2), interpolation='bilinear', data_format=IMAGE_ORDERING)(predict1)\n",
    "\n",
    "    ## (None, 224, 224, 10)\n",
    "    ## 2 times upsampling for pool411\n",
    "\n",
    "    predict2 = ( Conv2D( 1 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"predict2\", data_format=IMAGE_ORDERING))(pool4)\n",
    "#     concat1= Concatenate(name='concat1')([predict2, Deconv1])\n",
    "#     add_1 = Conv2D( 2 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"add_1\", data_format=IMAGE_ORDERING)(concat1)\n",
    "    add_1 = Add(name=\"add_1\")([predict2, Deconv1 ])\n",
    "#     Deconv2 = (Conv2DTranspose( nClasses , kernel_size=(3,3) , padding = 'same', strides=(2,2) , use_bias=False, data_format=IMAGE_ORDERING ))(add_1)\n",
    "    Deconv2 = (Conv2DTranspose( 1 , kernel_size=(4,4) , padding = 'same', strides=(2,2) , use_bias=False, kernel_initializer=my_init, data_format=IMAGE_ORDERING ))(add_1)\n",
    "    #     Deconv2 = UpSampling2D(size=(2, 2), interpolation='bilinear', data_format=IMAGE_ORDERING)(add_1)\n",
    "\n",
    "    predict3 = ( Conv2D( 1 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"predict3\", data_format=IMAGE_ORDERING))(pool3)\n",
    "#     concat2= Concatenate(name='concat2')([predict3, Deconv2])\n",
    "#     add_2 = Conv2D( 2 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"add_2\", data_format=IMAGE_ORDERING)(concat2)\n",
    "    add_2 = Add(name=\"add_2\")([predict3, Deconv2 ])\n",
    "#     Deconv3 = Conv2DTranspose( nClasses , kernel_size=(3,3) , padding = 'same', strides=(2,2) , use_bias=False, data_format=IMAGE_ORDERING )(add_2)# deconv3\n",
    "    Deconv3 = Conv2DTranspose( 1 , kernel_size=(4,4) , padding = 'same', strides=(2,2) , use_bias=False, kernel_initializer=my_init, data_format=IMAGE_ORDERING )(add_2)# deconv3\n",
    "#     Deconv3 = UpSampling2D(size=(2, 2), interpolation='bilinear', data_format=IMAGE_ORDERING)(add_2)\n",
    "\n",
    "    predict4 = ( Conv2D( 1 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"predict4\", data_format=IMAGE_ORDERING))(pool2)\n",
    "#     concat3= Concatenate(name='concat3')([predict4, Deconv3])\n",
    "#     add_3 = Conv2D( 2 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"add_3\", data_format=IMAGE_ORDERING)(concat3)\n",
    "    add_3 = Add(name=\"add_3\")([predict4, Deconv3 ])\n",
    "#     Deconv4 = Conv2DTranspose( nClasses , kernel_size=(3,3) , padding = 'same', strides=(2,2) , use_bias=False, data_format=IMAGE_ORDERING )(add_3)# deconv3\n",
    "    Deconv4 = Conv2DTranspose( 1 , kernel_size=(4,4) , padding = 'same', strides=(2,2) , use_bias=False, kernel_initializer=my_init, data_format=IMAGE_ORDERING )(add_3)# deconv3\n",
    "#     Deconv4 = UpSampling2D(size=(2, 2), interpolation='bilinear', data_format=IMAGE_ORDERING)(add_3)\n",
    "\n",
    "    predict5 = ( Conv2D( 1 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"predict5\", data_format=IMAGE_ORDERING))(pool1)\n",
    "\n",
    "#     concat4= Concatenate(name='concat4')([predict5, Deconv4])\n",
    "#     add_4 = Conv2D( 2 , ( 1 , 1 ) , activation='relu' , padding='same', name=\"add_4\", data_format=IMAGE_ORDERING)(concat4)\n",
    "    add_4 = Add(name=\"add_4\")([predict5, Deconv4 ])\n",
    "    Deconv5 = Conv2DTranspose( 1 , kernel_size=(4,4) , padding = 'same', strides=(2,2) , use_bias=False, kernel_initializer=my_init, data_format=IMAGE_ORDERING )(add_4)# deconv3\n",
    "#     Deconv5 = UpSampling2D(size=(2, 2), interpolation='bilinear', data_format=IMAGE_ORDERING)(add_4)\n",
    "\n",
    "#     o = Add(name=\"add\")([predict2, predict3, Deconv1 ])# predict2, predict3,Deconv1\n",
    "#     o = Conv2DTranspose( nClasses , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False, data_format=IMAGE_ORDERING )(o)# deconv3\n",
    "    o = (Activation('sigmoid'))(Deconv5)#sigmoid\n",
    "\n",
    "    model = Model(img_input, o)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cut chips functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### this function is to read the data and the relevant properties\n",
    "import sys\n",
    "def ReadData_geoinf(path):\n",
    "    \"\"\"\n",
    "    This function is used to read geoinformation\n",
    "\n",
    "    param path: img path\n",
    "\n",
    "    \"\"\"\n",
    "    ds = gdal.Open(path, 0)\n",
    "    if ds is None:\n",
    "        sys.exit('Could not open {0}.'.format(path))\n",
    "\n",
    "    geoTransform = ds.GetGeoTransform()\n",
    "    proj = ds.GetProjection()\n",
    "\n",
    "    XSize = ds.RasterXSize\n",
    "    YSize = ds.RasterYSize\n",
    "    MinX = geoTransform[0]\n",
    "    MaxY = geoTransform[3]\n",
    "    MaxX = MinX + geoTransform[1] * XSize\n",
    "    MinY = MaxY + geoTransform[5] * YSize\n",
    "\n",
    "    resolution = geoTransform[1]\n",
    "\n",
    "    data = ds.ReadAsArray()\n",
    "    res = {'data': data,\n",
    "           'geoTransform': geoTransform,\n",
    "           'projection': proj,\n",
    "           'minX': MinX,\n",
    "           'maxX': MaxX,\n",
    "           'minY': MinY,\n",
    "           'maxY': MaxY,\n",
    "           'Xsize': XSize,\n",
    "           'Ysize': YSize,\n",
    "           'resolution': resolution}\n",
    "    return res\n",
    "## This function is to cut np.ndarray into chips.\n",
    "def cut_array(data, row, col, row_buffer, col_buffer):\n",
    "    ### calculate the right low corner index for row and col\n",
    "    ### (row, col)\n",
    "    ### the index for numpy array will be [a, b)\n",
    "    data_row = data.shape[0]\n",
    "    data_col = data.shape[1]\n",
    "    if ((data_row - row) % (row - row_buffer) == 0):\n",
    "        row_list = list(range(row, data_row + 1, row - row_buffer))\n",
    "    else:\n",
    "        row_list = list(range(row, data_row + 1, row - row_buffer))\n",
    "        row_list.append(data_row)\n",
    "    if ((data_col - col) % (col - col_buffer) == 0):\n",
    "        col_list = list(range(col, data_col + 1, col - col_buffer))\n",
    "    else:\n",
    "        col_list = list(range(col, data_col + 1, col - col_buffer))\n",
    "        col_list.append(data_col)\n",
    "    res = []\n",
    "    for j in col_list:\n",
    "        for i in row_list:\n",
    "            res.append(data[i - row:i, j - col:j])\n",
    "\n",
    "    return np.array(res)\n",
    "### index is from [0, row_num*col_num)\n",
    "### return (minX, maxX, minY, maxY)\n",
    "### This function is to return the coordinate range for the index\n",
    "def chip_index_finder(index, row, col, data_row, data_col, row_buffer, col_buffer):\n",
    "    if((data_row-row)%(row-row_buffer)>0):\n",
    "        row_num = int((data_row-row)/(row-row_buffer))+2\n",
    "    else:\n",
    "        row_num = int((data_row-row)/(row-row_buffer))+1\n",
    "    if((data_col-col)%(col-col_buffer)>0):\n",
    "        col_num = int((data_col-col)/(col-col_buffer))+2\n",
    "    else:\n",
    "        col_num = int((data_col-col)/(col-col_buffer))+1\n",
    "    row_index = index%row_num\n",
    "    col_index = int(index/row_num)\n",
    "    if(row_index == row_num-1):\n",
    "        row_coor = data_row\n",
    "    elif(row_index == 0):\n",
    "        row_coor = row\n",
    "    else:\n",
    "        row_coor = row+(row_index)*(row-row_buffer)\n",
    "    if(col_index == col_num-1):\n",
    "        col_coor = data_col\n",
    "    elif(col_index == 0):\n",
    "        col_coor = col\n",
    "    else:\n",
    "        col_coor = col + col_index*(col-col_buffer)\n",
    "    return (row_coor - row, row_coor, col_coor - col, col_coor)\n",
    "def mosaic_chips(data_array, index_list, weight_array, data_row, data_col, row, col, row_buffer, col_buffer):\n",
    "    res = np.zeros((data_row, data_col))\n",
    "    for i in range(len(index_list)):\n",
    "        chip_index = index_list[i]\n",
    "        coors = chip_index_finder(chip_index, row, col, data_row, data_col, row_buffer, col_buffer)\n",
    "        temp_array = weight_array[i,:,:]*data_array[i,:,:]\n",
    "        res[coors[0]:coors[1],coors[2]:coors[3]] = res[coors[0]:coors[1],coors[2]:coors[3]]+temp_array\n",
    "    return res\n",
    "def weights_generator(weight_type, data_row, data_col, row, col, row_buffer, col_buffer):\n",
    "    if ((data_row - row) % (row - row_buffer) > 0):\n",
    "        row_num = int((data_row - row) / (row - row_buffer)) + 2\n",
    "    else:\n",
    "        row_num = int((data_row - row) / (row - row_buffer)) + 1\n",
    "    if ((data_col - col) % (col - col_buffer) > 0):\n",
    "        col_num = int((data_col - col) / (col - col_buffer)) + 2\n",
    "    else:\n",
    "        col_num = int((data_col - col) / (col - col_buffer)) + 1\n",
    "    if (weight_type == 'no_buffer'):\n",
    "        weight_array = [np.ones((row, col)) for _ in range(row_num * col_num)]\n",
    "        right_margin = np.zeros((row, col))\n",
    "        right_cut = (row if (data_row % row == 0) else data_row % row)\n",
    "        right_margin[row - right_cut:, :] = 1\n",
    "        down_margin = np.zeros((row, col))\n",
    "        down_cut = (col if (data_col % col == 0) else data_col % col)\n",
    "        down_margin[:, col - down_cut:] = 1\n",
    "        corner_margin = right_margin * down_margin\n",
    "        for i in range(row_num - 1, row_num * col_num, row_num):\n",
    "            weight_array[i] = right_margin\n",
    "        for i in range(row_num * col_num - row_num, row_num * col_num):\n",
    "            weight_array[i] = down_margin\n",
    "        weight_array[row_num * col_num - 1] = corner_margin\n",
    "        weight_array = np.array(weight_array)\n",
    "    if (weight_type == 'buffer_average'):\n",
    "        weight_array = []\n",
    "        template_zeros = np.zeros((data_row, data_col))\n",
    "        for i in range(col_num * row_num):\n",
    "            coors = chip_index_finder(i, row, col, data_row, data_col, row_buffer, col_buffer)\n",
    "            temp_array = np.ones((row, col))\n",
    "            template_zeros[coors[0]:coors[1], coors[2]:coors[3]] = template_zeros[coors[0]:coors[1],\n",
    "                                                                   coors[2]:coors[3]] + temp_array\n",
    "        template_zeros = 1.0 / template_zeros\n",
    "        for i in range(col_num * row_num):\n",
    "            coors = chip_index_finder(i, row, col, data_row, data_col, row_buffer, col_buffer)\n",
    "            weight_array.append(template_zeros[coors[0]:coors[1], coors[2]:coors[3]])\n",
    "        weight_array = np.array(weight_array)\n",
    "    if (weight_type == 'buffer_gauss_average'):\n",
    "        def gaus2d(x=0, y=0, mx=0, my=0, sx=1, sy=1):\n",
    "            return 1. / (2. * np.pi * sx * sy) * np.exp(\n",
    "                -((x - mx) ** 2. / (2. * sx ** 2.) + (y - my) ** 2. / (2. * sy ** 2.)))\n",
    "\n",
    "        weight_array = []\n",
    "        x = np.linspace(-5, 5, row)\n",
    "        y = np.linspace(-5, 5, col)\n",
    "        x, y = np.meshgrid(x, y)  # get 2D variables instead of 1D\n",
    "        template_weights = gaus2d(x, y)\n",
    "        weight_array = []\n",
    "        template_zeros = np.zeros((data_row, data_col))\n",
    "        for i in range(col_num * row_num):\n",
    "            coors = chip_index_finder(i, row, col, data_row, data_col, row_buffer, col_buffer)\n",
    "            template_zeros[coors[0]:coors[1], coors[2]:coors[3]] = template_zeros[coors[0]:coors[1],\n",
    "                                                                   coors[2]:coors[3]] + template_weights\n",
    "\n",
    "        for i in range(col_num * row_num):\n",
    "            coors = chip_index_finder(i, row, col, data_row, data_col, row_buffer, col_buffer)\n",
    "            weight_array.append(template_weights / template_zeros[coors[0]:coors[1], coors[2]:coors[3]])\n",
    "        weight_array = np.array(weight_array)\n",
    "    if (weight_type == 'buffer_linear_average'):\n",
    "        assert row_buffer == col_buffer\n",
    "        template_weights = np.ones((row, col))\n",
    "        for i in range(row_buffer):\n",
    "            pixel_int = 1. / row_buffer\n",
    "            template_weights[i, i:(col - i)] = pixel_int * i\n",
    "            template_weights[i:(row - i), i] = pixel_int * i\n",
    "            template_weights[row - i - 1, i:(col - i)] = pixel_int * i\n",
    "            template_weights[i:(row - i), col - i - 1] = pixel_int * i\n",
    "        weight_array = []\n",
    "        template_zeros = np.zeros((data_row, data_col))\n",
    "        for i in range(col_num * row_num):\n",
    "            coors = chip_index_finder(i, row, col, data_row, data_col, row_buffer, col_buffer)\n",
    "            template_zeros[coors[0]:coors[1], coors[2]:coors[3]] = template_zeros[coors[0]:coors[1],\n",
    "                                                                   coors[2]:coors[3]] + template_weights\n",
    "\n",
    "        for i in range(col_num * row_num):\n",
    "            coors = chip_index_finder(i, row, col, data_row, data_col, row_buffer, col_buffer)\n",
    "            weight_array.append(template_weights / template_zeros[coors[0]:coors[1], coors[2]:coors[3]])\n",
    "        weight_array = np.array(weight_array)\n",
    "\n",
    "    if (weight_type == 'half_buffer'):\n",
    "        half_row_buffer = int(row_buffer / 2)\n",
    "        half_col_buffer = int(col_buffer / 2)\n",
    "        template_weights = np.zeros((row, col))\n",
    "        template_weights[half_row_buffer:(row - half_row_buffer), half_col_buffer:(col - half_col_buffer)] = 1\n",
    "        weight_array = [template_weights for _ in range(row_num * col_num)]\n",
    "\n",
    "        up_border = np.zeros((row, col))\n",
    "        up_border[half_row_buffer:(row - half_row_buffer), 0:(col - half_col_buffer)] = 1\n",
    "        down_border = np.zeros((row, col))\n",
    "        down_border[half_row_buffer:(row - half_row_buffer), half_col_buffer:] = 1\n",
    "        left_border = np.zeros((row, col))\n",
    "        left_border[0:(row - half_row_buffer), half_col_buffer:(col - half_col_buffer)] = 1\n",
    "        right_border = np.zeros((row, col))\n",
    "        right_border[half_row_buffer:, half_col_buffer:(col - half_col_buffer)] = 1\n",
    "\n",
    "        up_left_border = np.zeros((row, col))\n",
    "        up_left_border[0:(row - half_row_buffer), 0:(col - half_col_buffer)] = 1\n",
    "\n",
    "        up_right_border = np.zeros((row, col))\n",
    "        up_right_border[half_row_buffer:, 0:(col - half_col_buffer)] = 1\n",
    "        down_left_border = np.zeros((row, col))\n",
    "        down_left_border[0:(row - half_row_buffer), half_col_buffer:] = 1\n",
    "        down_right_border = np.zeros((row, col))\n",
    "        down_right_border[half_row_buffer:, half_col_buffer:] = 1\n",
    "\n",
    "        for i in range(0, row_num):\n",
    "            weight_array[i] = up_border\n",
    "        for i in range(0, row_num * col_num - 1, row_num):\n",
    "            weight_array[i] = left_border\n",
    "\n",
    "        right_cut = ((row - row_buffer) if ((data_row - row) % (row - row_buffer) == 0) else (data_row - row) % (\n",
    "                    row - row_buffer))\n",
    "        down_cut = ((col - col_buffer) if ((data_col - col) % (col - col_buffer) == 0) else (data_col - col) % (\n",
    "                    col - col_buffer))\n",
    "        #         half_row_buffer:(row - half_row_buffer), half_col_buffer:(col - half_col_buffer)\n",
    "        if (right_cut == (row - row_buffer)):\n",
    "            for i in range(row_num - 1, row_num * col_num - 1, row_num):\n",
    "                weight_array[i] = right_border\n",
    "            if (down_cut == (col - col_buffer)):\n",
    "                for i in range(row_num * col_num - row_num, row_num * col_num - 1):\n",
    "                    weight_array[i] = down_border\n",
    "                weight_array[0] = up_left_border\n",
    "                weight_array[row_num - 1] = up_right_border\n",
    "                weight_array[row_num * (col_num - 1)] = down_left_border\n",
    "                weight_array[row_num * col_num - 1] = down_right_border\n",
    "            else:\n",
    "                down_margin = np.zeros((row, col))\n",
    "                down_margin[half_row_buffer:(row - half_row_buffer), col - down_cut:] = 1\n",
    "                left_down_margin = np.zeros((row, col))\n",
    "                left_down_margin[0:(row - half_row_buffer), col - down_cut:] = 1\n",
    "                right_down_margin_2 = np.zeros((row, col))\n",
    "                right_down_margin_2[half_row_buffer:(row), col - down_cut:] = 1\n",
    "                for i in range(row_num * col_num - row_num, row_num * col_num - 1):\n",
    "                    weight_array[i] = down_margin\n",
    "                    weight_array[i - row_num] = down_border\n",
    "                weight_array[0] = up_left_border\n",
    "                weight_array[row_num - 1] = up_right_border\n",
    "                weight_array[row_num * (col_num - 2)] = down_left_border\n",
    "                weight_array[row_num * (col_num - 1)] = left_down_margin\n",
    "                weight_array[row_num * col_num - row_num - 1] = down_right_border\n",
    "                weight_array[row_num * col_num - 2] = right_down_margin_2\n",
    "        else:\n",
    "            right_margin = np.zeros((row, col))\n",
    "            right_margin[row - right_cut:, half_col_buffer:(col - half_col_buffer)] = 1\n",
    "            right_up_margin = np.zeros((row, col))\n",
    "            right_up_margin[row - right_cut:, 0:(col - half_col_buffer)] = 1\n",
    "            right_down_margin = np.zeros((row, col))\n",
    "            right_down_margin[row - right_cut:, half_col_buffer:col] = 1\n",
    "            for i in range(row_num - 1, row_num * col_num - 1, row_num):\n",
    "                weight_array[i] = right_margin\n",
    "                weight_array[i - 1] = right_border\n",
    "\n",
    "            if (down_cut == (col - col_buffer)):\n",
    "                for i in range(row_num * col_num - row_num, row_num * col_num - 1):\n",
    "                    weight_array[i] = down_border\n",
    "                weight_array[0] = up_left_border\n",
    "                weight_array[row_num - 2] = up_right_border\n",
    "                weight_array[row_num - 1] = right_up_margin\n",
    "                weight_array[row_num * col_num - row_num - 1] = right_down_margin\n",
    "                weight_array[row_num * (col_num - 1)] = down_left_border\n",
    "                weight_array[row_num * col_num - 2] = down_right_border\n",
    "            else:\n",
    "                down_margin = np.zeros((row, col))\n",
    "                down_margin[half_row_buffer:(row - half_row_buffer), col - down_cut:] = 1\n",
    "                left_down_margin = np.zeros((row, col))\n",
    "                left_down_margin[0:(row - half_row_buffer), col - down_cut:] = 1\n",
    "                right_down_margin_2 = np.zeros((row, col))\n",
    "                right_down_margin_2[half_row_buffer:(row), col - down_cut:] = 1\n",
    "                for i in range(row_num * col_num - row_num, row_num * col_num - 1):\n",
    "                    weight_array[i] = down_margin\n",
    "                    weight_array[i - row_num] = down_border\n",
    "                weight_array[0] = up_left_border\n",
    "                weight_array[row_num - 2] = up_right_border\n",
    "                weight_array[row_num - 1] = right_up_margin\n",
    "                weight_array[row_num * col_num - row_num - 1] = right_down_margin\n",
    "                weight_array[row_num * (col_num - 2)] = down_left_border\n",
    "                weight_array[row_num * (col_num - 1)] = left_down_margin\n",
    "                weight_array[row_num * col_num - row_num - 2] = down_right_border\n",
    "                corner_margin = np.zeros((row, col))\n",
    "                corner_margin[row - right_cut:, col - down_cut:] = 1\n",
    "                weight_array[row_num * col_num - 2] = right_down_margin_2\n",
    "                weight_array[row_num * col_num - 1] = corner_margin\n",
    "                print(corner_margin)\n",
    "        weight_array = np.array(weight_array)\n",
    "\n",
    "    return weight_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## output the data in the same format\n",
    "def output_same(data, template_file_name, output_name, gdal_type):\n",
    "    gtif = gdal.Open(template_file_name)\n",
    "    ## get the first band in the file\n",
    "    band = gtif.GetRasterBand(1)\n",
    "    ## get the rows and cols of the input file\n",
    "    rows = gtif.RasterYSize\n",
    "    cols = gtif.RasterXSize\n",
    "    output_format = output_name.split('.')[-1].upper()\n",
    "    if (output_format == 'TIF'):\n",
    "        output_format = 'GTIFF'\n",
    "    elif (output_format == 'RST'):\n",
    "        output_format = 'rst'\n",
    "    driver = gdal.GetDriverByName(output_format)\n",
    "    outDs = driver.Create(output_name, cols, rows, 1, gdal_type)\n",
    "    outBand = outDs.GetRasterBand(1)\n",
    "    outBand.WriteArray(data)\n",
    "    # georeference the image and set the projection\n",
    "    outDs.SetGeoTransform(gtif.GetGeoTransform())\n",
    "    outDs.SetProjection(gtif.GetProjection())\n",
    "    outDs.FlushCache()\n",
    "    outBand.SetNoDataValue(-99)\n",
    "    ## need to release the driver\n",
    "    del outDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint,  CSVLogger, EarlyStopping\n",
    "import datetime\n",
    "project_folder = '/home/vishal/DL_Results/unet_datagen_bs64_dropout_1000ss'\n",
    "if(not os.path.isdir(project_folder)):\n",
    "    os.mkdir(project_folder)\n",
    "\n",
    "log_dir=\"/home/vishal/DL_Results/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "checkpoint_path = os.path.join(project_folder,'saved_models','weight_improvements_{epoch:02d}_{val_accuracy:.2f}.hdf5')\n",
    "if(not os.path.isdir(os.path.join(project_folder,'saved_models'))):\n",
    "    os.mkdir(os.path.join(project_folder,'saved_models'))\n",
    "# checkpoint = ModelCheckpoint(checkpoint_path, monitor = 'val_iou_score', verbose = 1,save_best_only=True,mode='max')\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, period=50)\n",
    "#early_stop = EarlyStopping(monitor = 'val_loss', patience = 3, verbose=1)\n",
    "log_csv = CSVLogger(os.path.join(project_folder,'my_logs.csv'),separator=',',append=False)\n",
    "callback_list = [ checkpoint,log_csv]\n",
    "batch_size = 64\n",
    "epoch_num = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Read sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "type_list = ['ecuador extensive', 'intensive', 'long lot extensive', 'smallholder extensive AP']\n",
    "source_folder = '/workspace/_libs/dl_library'\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "x_val=[]\n",
    "y_val=[]\n",
    "for i in os.listdir(source_folder):\n",
    "    if(i in type_list):\n",
    "        bands, label = read_scenes([os.path.join(source_folder, i)])\n",
    "        type_size = bands.shape[0]\n",
    "        random_list = random.sample(range(type_size), 1000)\n",
    "        sample_bands = bands[random_list,:,:,:]\n",
    "        sample_label = label[random_list,:,:]\n",
    "        x_train_type, x_val_type, y_train_type, y_val_type = train_test_split(sample_bands, sample_label, test_size=0.2)\n",
    "        x_train.append(x_train_type)\n",
    "        x_val.append(x_val_type)\n",
    "        y_train.append(y_train_type)\n",
    "        y_val.append(y_val_type)\n",
    "x_train = np.concatenate(x_train)\n",
    "y_train = np.concatenate(y_train)\n",
    "x_val = np.concatenate(x_val)\n",
    "y_val = np.concatenate(y_val)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(y_train)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(x_val)\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.expand_dims(y_train, axis = -1)\n",
    "y_val = np.expand_dims(y_val, axis = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25698/2589378913.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_gen = data_generator(x_train,y_train,64)\n",
    "val_gen = data_generator(x_val,y_val,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### offline augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clarkAug' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25698/3267635167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclarkAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclarkAug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clarkAug' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, y_train = clarkAug(x_train, y_train)\n",
    "x_val, y_val = clarkAug(x_val, y_val)\n",
    "y_train = np.expand_dims(y_train, axis = -1)\n",
    "y_val = np.expand_dims(y_val, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import segmentation_models\n",
    "import os\n",
    "## two GPUs, so the devices are gpu0, and gpu1. If you try more GPUs, remember to change the devices.\n",
    "strategy = tf.distribute.MirroredStrategy(devices= [\"/gpu:0\",\"/gpu:1\",\"/gpu:2\",\"/gpu:3\"],cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "# read autoencoder\n",
    "# autoencoder_model_unet = load_model('/workspace/_libs/pre_trained_RS_autoencoder.h5')\n",
    "#autoencoder_model_FCN = load_model('/workspace/_libs/FCN_autoencoder_model.h5')\n",
    "loss = segmentation_models.losses.BinaryFocalLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Required Callback Function\n",
    "# class printlearningrate(tf.keras.callbacks.Callback):\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         optimizer = self.model.optimizer\n",
    "#         lr = K.eval(optimizer.lr)\n",
    "#         Epoch_count = epoch + 1\n",
    "#         print('\\n', \"Epoch:\", Epoch_count, ', LR: {:.2f}'.format(lr))\n",
    "\n",
    "# printlr = printlearningrate() \n",
    "\n",
    "def scheduler(epoch):\n",
    "  optimizer = model.optimizer\n",
    "  return K.eval(optimizer.lr - 0.01)\n",
    "\n",
    "updatelr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3E-4\n",
    "for i in range(500):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  model_1 = binary_unet(256,256,6)\n",
    "  # model_1 = binary_unet(256,256,6)\n",
    "\n",
    "  # for l1,l2 in zip(autoencoder_model_unet.layers[:35], model_1.layers[:35]):\n",
    "  #     l2.set_weights(l1.get_weights())\n",
    "  #for l1,l2 in zip(autoencoder_model_FCN.layers[:47], model_1.layers[:47]):\n",
    "  #    l2.set_weights(l1.get_weights())\n",
    "    \n",
    "\n",
    "  model_1.compile(loss=loss, # 'weighted_categorical_crossentropy'\n",
    "              optimizer = keras.optimizers.Adam(lr=3E-4),\n",
    "              metrics=['accuracy',segmentation_models.metrics.IOUScore()])\n",
    "\n",
    "# hist = model_1.fit(x_train, y_train,\n",
    "#                    validation_data=(x_val, y_val),\n",
    "#                    batch_size=batch_size,\n",
    "#                    epochs=epoch_num, verbose=1\n",
    "#                    )\n",
    "# import segmentation_models\n",
    "# from tensorflow.keras import optimizers\n",
    "# loss = segmentation_models.losses.BinaryFocalLoss()\n",
    "# sgd = optimizers.SGD(lr=1E-4, decay=5**(-4), momentum=0.9, nesterov=True)\n",
    "# adam_opt = optimizers.Adam(lr=1E-4)\n",
    "# model_1.compile(loss=loss, # 'weighted_categorical_crossentropy'\n",
    "#               optimizer = adam_opt,\n",
    "#               metrics=['accuracy'])\n",
    "hist1 = model_1.fit_generator(train_gen,\n",
    "                        steps_per_epoch=len(x_train) // 64,\n",
    "                        validation_data=val_gen,\n",
    "                        validation_steps=len(x_val)//64,\n",
    "                        epochs = 500, verbose=1, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key in ['loss', 'val_loss']:\n",
    "    plt.plot(hist1.history[key],label=key)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(project_folder,'loss.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key in ['accuracy', 'val_accuracy']:\n",
    "    plt.plot(hist1.history[key],label=key)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(project_folder,'accuracy.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for key in ['iou_score', 'val_iou_score']:\n",
    "    plt.plot(hist1.history[key],label=key)\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(project_folder,'IoU.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(project_folder, \"unet_64_1000_lr3\"+\".h5\")\n",
    "model_1.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### predict scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist1.history.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# when you have customized initializer, set custom objects before load the model (https://github.com/keras-team/keras/issues/3867)\n",
    "from tensorflow.keras.utils import CustomObjectScope, get_custom_objects\n",
    "class CustomInitializer:\n",
    "    def __call__(self, shape, dtype=None):\n",
    "        return my_init(shape, dtype=dtype)\n",
    "\n",
    "get_custom_objects().update({'my_init': CustomInitializer})\n",
    "\n",
    "get_custom_objects().update({\"binary_focal_loss\": segmentation_models.losses.BinaryFocalLoss()})\n",
    "get_custom_objects().update({\"iou_score\": segmentation_models.metrics.IOUScore()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model_final = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scene_dict = {'116060':'/home/zhen/PycharmProjects/DLrepo/data/116060_preprocessed'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_ID = '116060'\n",
    "sourcefolder = scene_dict[predict_ID]\n",
    "folderlist = []\n",
    "## get bands 2~7\n",
    "for i in os.listdir(sourcefolder):\n",
    "    if(i.endswith('rst')):\n",
    "        folderlist.append(os.path.join(sourcefolder,i))\n",
    "folderlist.sort()\n",
    "folderlist\n",
    "predict_data_source = []\n",
    "for i in folderlist:\n",
    "    ds = ReadData_geoinf(i)\n",
    "    predict_data_source.append(ds['data'])\n",
    "predict_data_source = np.stack(predict_data_source,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_chips = cut_array(predict_data_source, 256, 256, 128, 128)\n",
    "predict_label = model_final.predict(predict_chips)\n",
    "mosaic_weights = weights_generator('half_buffer', ds['Ysize'], ds['Xsize'], 256, 256, 128, 128)\n",
    "mosaic_res =  mosaic_chips(predict_label[:,:,:,0], range(len(predict_label)),mosaic_weights, ds['Ysize'], ds['Xsize'], 256, 256, 128, 128)\n",
    "prediction_path = os.path.join(project_folder, project_name+'_prob_'+predict_ID+'.rst')\n",
    "output_same(mosaic_res, os.path.join(sourcefolder,folderlist[0]), prediction_path, gdal.GDT_Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
